Loss: 0.18422051349824126
Accuracy: 0.644715909090909
----evaluation finished----
----args----
hiddensize: 12, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----



-------------------------------
Loss: 0.18150999092242934
Accuracy: 0.6530397727272728
----evaluation finished----
----args----
hiddensize: 12, layers:  1
epochs: 80, learn rate: 0.001
batch size: 32, window size: 5
----end----



总结：经过实验，可以通过增大hidden size的方式构成较大模型，增大参数量，对于这些大模型而言：可以通过多epoch对其进行充分训练，使训练集对应的loss非常低，但这种情况下，测试集效果较差，发生过拟合。在训练机器模型的过程中，我们面临的最大问题是数据量太小，且难以进行数据增强。所以我们选择使用小模型进行预测，虽然在训练过程中无法得到满意的loss，但测试集与训练集的结果差异小，使得最终的准确率较高。关于layer，在我们的实验中layer的影响非常小。关于windowSize，影响非常小

图：
epoch（说明大模型缺陷
layer（影响小
梯度消失/爆炸问题：虽然 LSTM 设计的初衷就是为了解决梯度消失问题，但在深度 LSTM 网络中，这个问题仍然可能出现。当 LSTM 层数增加时，梯度可能在反向传播过程中消失或爆炸，导致模型难以学习。

过拟合：增加 LSTM 层数会增加模型的复杂性，可能导致模型在训练数据上过拟合，从而在测试数据上的性能下降。

计算成本：增加 LSTM 层数会显著增加模型的计算成本。如果没有足够的计算资源，模型可能无法充分训练。

数据复杂性：如果数据集的复杂性较低，那么增加 LSTM 层数可能不会带来显著的性能提升，因为一个简单的模型就足以捕获数据的模式。）
windowSize（影响小，在用----方法概括后，lstm模型无法从之前的数据中获得信息，而在model3中，windowSize产生影响）

-------------------------------------------------------------from here
Loss: 0.655711873688481
Accuracy: 0.6505965909090908
F1: 0.6461420057922509
F2: 0.6251216655097281
F0.5: 0.6686251957231398
----evaluation finished----
----args----
hiddensize: 32, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

-----------------------------------------------------------------------------------------layer（把hiddensize都当成5）


-------------------------------------------------
Loss: 0.6541476371613416
Accuracy: 0.6498863636363637
F1: 0.6466010924202473
F2: 0.6267903755627241
F0.5: 0.6677049811222642
----evaluation finished----
----args----
hiddensize: 32, layers:  2
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

------------------------------------------------
Loss: 0.6561981216073036
Accuracy: 0.6484659090909091
F1: 0.6428799589001014
F2: 0.6190344626079207
F0.5: 0.6686361347431485
----evaluation finished----
----args----
hiddensize: 32, layers:  3
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

-------------------------------------------------
Loss: 0.6519088731570677
Accuracy: 0.6498863636363637
F1: 0.6451167497809491
F2: 0.6231878920955127
F0.5: 0.6686451669635992
----evaluation finished----
----args----
hiddensize: 32, layers:  5
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

---------------------------------------------------------------------------hiddensize
Loss: 0.6533525789325888
Accuracy: 0.6491761363636364
F1: 0.6454028691420607
F2: 0.6248447402760486
F0.5: 0.6673597933958619
----evaluation finished----
----args----
hiddensize: 16, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5

-----------------------------------------------------
Loss: 0.6527301249178973
Accuracy: 0.6513068181818181
F1: 0.6470142881596984
F2: 0.6262389633812167
F0.5: 0.6692153438727326
----evaluation finished----
----args----
hiddensize: 8, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

---------------------------------------------------------
Loss: 0.6508795456452803
Accuracy: 0.6520170454545454
F1: 0.6479900580426462
F2: 0.6273533384943266
F0.5: 0.6700306428566177
----evaluation finished----
----args----
hiddensize: 5, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

----------------------------------------------------------
Loss: 0.6701345362446525
Accuracy: 0.6328409090909091
F1: 0.6301984600210722
F2: 0.6122041648307024
F0.5: 0.6492825878646564
----evaluation finished----
----args----
hiddensize: 128, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 5
----end----

---------------------------------------------------------------------------------------------epochs（大模型）
Loss: 3.220782997996308
Accuracy: 0.5572674418604651
F1: 0.565926761313071
F2: 0.5635711827665005
F0.5: 0.5683021139010164
----evaluation finished----
----args----
hiddensize: 64, layers:  1
epochs: 500, learn rate: 0.001
batch size: 32, window size: 10
----end----

Accuracy: 0.5983042635658915
F1: 0.6055875786613679
F2: 0.5999523818288772
F0.5: 0.6113296391153141
----evaluation finished----
----args----
hiddensize: 64, layers:  1
epochs: 100, learn rate: 0.001
batch size: 32, window size: 10
----end----

Loss: 0.6853500521460245
Accuracy: 0.6222383720930232
F1: 0.6217011717580238
F2: 0.6056965770129387
F0.5: 0.6385745151402169
----evaluation finished----
----args----
hiddensize: 64, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 10
----end----

Loss: 0.6562751947447311
Accuracy: 0.6449127906976744
F1: 0.652703026854157
F2: 0.6400874614239279
F0.5: 0.6658258739399966
----evaluation finished----
----args----
hiddensize: 64, layers:  1
epochs: 30, learn rate: 0.001
batch size: 32, window size: 10
----end----

---------------------------------------------------------window
Loss: 0.6528252917666768
Accuracy: 0.6441860465116279
F1: 0.6463080012558032
F2: 0.6254996589568926
F0.5: 0.6685484365823486
----evaluation finished----
----args----
hiddensize: 5, layers:  3
epochs: 50, learn rate: 0.001
batch size: 32, window size: 10
----end----

-----------------------------------------------------
Accuracy: 0.6501524390243902
F1: 0.6502671187782195
F2: 0.6280739806613834
F0.5: 0.6740861058900638
----evaluation finished----
----args----
hiddensize: 5, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 20
----end----

-------------------------------------------------------
Loss: 0.6544244933772732
Accuracy: 0.6493725868725869
F1: 0.6485787824102287
F2: 0.6249339186807025
F0.5: 0.6740832495241312
----evaluation finished----
----args----
hiddensize: 5, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 40
----end----

---------------------------------------------------
Accuracy: 0.6526785714285714
F1: 0.6454406196915459
F2: 0.6231263972735769
F0.5: 0.6694123468770848
----evaluation finished----
----args----
hiddensize: 5, layers:  1
epochs: 50, learn rate: 0.001
batch size: 32, window size: 1
----end----